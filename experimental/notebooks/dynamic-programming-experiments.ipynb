{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming Algorithms - Experiments - Chapter 5 and 6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from typing import List, Optional\n",
    "from shapely.geometry import Polygon, MultiPolygon, Point\n",
    "\n",
    "from experimental.utils.partition import Partition\n",
    "from experimental.utils.dynamic_system import DynamicSystem\n",
    "from experimental.utils.markov_decision_process import MarkovDecisionProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"paper\")\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build equally-spaced state space partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load baseline partitions from disk, if they already exist\n",
    "if os.path.isfile(\"results/baseline_3_partitions.pkl\"):\n",
    "    with open(\"results/baseline_3_partitions.pkl\", \"rb\") as file:\n",
    "        baseline_partitions = pickle.load(file)\n",
    "else:\n",
    "    baseline_partitions = {}\n",
    "\n",
    "baseline_partitions.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "m_id = 1\n",
    "equally_spaced_offset_x = np.array([m_id/3, 0])\n",
    "equally_spaced_offset_y = np.array([0, m_id/3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u1 = np.array([0,0])\n",
    "u2 = np.array([0,1])\n",
    "u3 = np.array([1,1])\n",
    "u4 = np.array([1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = u1 + equally_spaced_offset_x\n",
    "x2 = u1 + 2*equally_spaced_offset_x\n",
    "x3 = u2 + equally_spaced_offset_x\n",
    "x4 = u2 + 2*equally_spaced_offset_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_vertices = [u1, u2, x3, x1]\n",
    "X1 = MultiPolygon([Polygon(X1_vertices)])\n",
    "\n",
    "X2_vertices = [x3, x4, x2, x1]\n",
    "X2 = MultiPolygon([Polygon(X2_vertices)])\n",
    "\n",
    "X3_vertices = [x4, u3, u4, x2]\n",
    "X3 = MultiPolygon([Polygon(X3_vertices)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MultiPolygon([Polygon([u1, u2, x3, x1]), Polygon([x3, x4, x2, x1]), Polygon([x4, u3, u4, x2])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizontal_partition = [X1, X2, X3]\n",
    "horizontal_partition_vertices = [X1_vertices, X2_vertices, X3_vertices]\n",
    "\n",
    "baseline_partitions[\"horizontal\"] = {}\n",
    "baseline_partitions[\"horizontal\"][\"partition\"] = horizontal_partition\n",
    "baseline_partitions[\"horizontal\"][\"partition_vertices\"] = horizontal_partition_vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = u1 + equally_spaced_offset_y\n",
    "y2 = u1 + 2*equally_spaced_offset_y\n",
    "y3 = u4 + equally_spaced_offset_y\n",
    "y4 = u4 + 2*equally_spaced_offset_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y1_vertices = [u1, y1, y3, u4]\n",
    "Y1 = MultiPolygon([Polygon(Y1_vertices)])\n",
    "\n",
    "Y2_vertices = [y1, y2, y4, y3]\n",
    "Y2 = MultiPolygon([Polygon(Y2_vertices)])\n",
    "\n",
    "Y3_vertices = [y2, u2, u3, y4]\n",
    "Y3 = MultiPolygon([Polygon(Y3_vertices)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MultiPolygon([Polygon([u1, y1, y3, u4]), Polygon([y1, y2, y4, y3]), Polygon([y2, u2, u3, y4])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertical_partition = [Y1, Y2, Y3]\n",
    "vertical_partition_vertices = [Y1_vertices, Y2_vertices, Y3_vertices]\n",
    "\n",
    "baseline_partitions[\"vertical\"] = {}\n",
    "baseline_partitions[\"vertical\"][\"partition\"] = vertical_partition\n",
    "baseline_partitions[\"vertical\"][\"partition_vertices\"] = vertical_partition_vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_halved = np.array([m_id/2, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1_vertices = [u2, u3, y4, y2]\n",
    "A1 = MultiPolygon([Polygon(A1_vertices)])\n",
    "\n",
    "A2_vertices = [u1, y2, y2+x_halved, u1+x_halved]\n",
    "A2 = MultiPolygon([Polygon(A2_vertices)])\n",
    "\n",
    "A3_vertices = [u1+x_halved, y2+x_halved, y4, u4]\n",
    "A3 = MultiPolygon([Polygon(A3_vertices)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MultiPolygon([Polygon([u2, u3, y4, y2]), Polygon([u1, y2, y2+x_halved, u1+x_halved]), Polygon([u1+x_halved, y2+x_halved, y4, u4])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_partition_1 = [A1, A2, A3]\n",
    "complex_partition_vertices_1 = [A1_vertices, A2_vertices, A3_vertices]\n",
    "\n",
    "baseline_partitions[\"complex_1\"] = {}\n",
    "baseline_partitions[\"complex_1\"][\"partition\"] = complex_partition_1\n",
    "baseline_partitions[\"complex_1\"][\"partition_vertices\"] = complex_partition_vertices_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_halved = np.array([0, m_id/2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B1_vertices = [u1, u1+y_halved, x2+y_halved, x2]\n",
    "B1 = MultiPolygon([Polygon(B1_vertices)])\n",
    "\n",
    "B2_vertices = [u1+y_halved, u2, x4, x2+y_halved]\n",
    "B2 = MultiPolygon([Polygon(B2_vertices)])\n",
    "\n",
    "B3_vertices = [x4, u3, u4, x2]\n",
    "B3 = MultiPolygon([Polygon(B3_vertices)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MultiPolygon([Polygon([u1, u1+y_halved, x2+y_halved, x2]), Polygon([u1+y_halved, u2, x4, x2+y_halved]), Polygon([x4, u3, u4, x2])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_partition_2 = [B1, B2, B3]\n",
    "complex_partition_vertices_2 = [B1_vertices, B2_vertices, B3_vertices]\n",
    "\n",
    "baseline_partitions[\"complex_2\"] = {}\n",
    "baseline_partitions[\"complex_2\"][\"partition\"] = complex_partition_2\n",
    "baseline_partitions[\"complex_2\"][\"partition_vertices\"] = complex_partition_vertices_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_partitions.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Policy Evaluation Algorithm with standard example dynamic system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1,1], [1,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi(x: np.array) -> np.array:\n",
    "    x_new = np.dot(A, x) % 1\n",
    "    return x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q(x: np.array) -> np.array:\n",
    "    return x % 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get eigenvalues and right eigenvectors, i.e. transposed eigenvectors\n",
    "eig_vals, eig_vects = np.linalg.eig(A)\n",
    "# retrieve eigenvectors from right eigenvectors\n",
    "eig_vects = np.transpose(eig_vects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Eigenvalues: {eig_vals}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Eigenvectors: {eig_vects}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = eig_vects[0]\n",
    "v2 = eig_vects[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perp(a) :\n",
    "    b = np.empty_like(a)\n",
    "    b[0] = -a[1]\n",
    "    b[1] = a[0]\n",
    "    return b\n",
    "\n",
    "# line segment a given by endpoints a1, a2\n",
    "# line segment b given by endpoints b1, b2\n",
    "def seg_intersect(a, b) :\n",
    "    a1, a2 = a\n",
    "    b1, b2 = b\n",
    "\n",
    "    da = a2 - a1\n",
    "    db = b2 - b1\n",
    "    dp = a1 - b1\n",
    "    dap = perp(da)\n",
    "    denom = np.dot(dap, db)\n",
    "    num = np.dot(dap, dp)\n",
    "\n",
    "    return (num / denom.astype(float))*db + b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u1 = np.array([0,0])\n",
    "u2 = np.array([0,1])\n",
    "u3 = np.array([1,1])\n",
    "u4 = np.array([1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lu1 = np.array([[0,0], [0,1]])\n",
    "lu2 = np.array([[0,1], [1,1]])\n",
    "lu3 = np.array([[1,1], [1,0]])\n",
    "lu4 = np.array([[1,0], [0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = u1 + (1/v1[0]) * v1\n",
    "p2 = u2 - (1/v2[1]) * v2\n",
    "p3 = u3 - (1/v1[0]) * v1\n",
    "p4 = u4 + (1/v2[1]) * v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_00 = np.array([u1, p1])\n",
    "l_01 = np.array([u2, p2])\n",
    "l_10 = np.array([u4, p4])\n",
    "l_11 = np.array([u3, p3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P1a = seg_intersect(l_00, l_01)\n",
    "P1b = seg_intersect(l_10, l_00)\n",
    "\n",
    "P3a = seg_intersect(l_10, l_11)\n",
    "\n",
    "symm_helper_x = seg_intersect(lu2, l_10)\n",
    "symm_helper_y = symm_helper_x - np.array([0,1])\n",
    "symm_helper_l11 = np.array([symm_helper_x, symm_helper_y])\n",
    "\n",
    "p1_symm = symm_helper_y + v2\n",
    "l_10_symm_extension = np.array([symm_helper_y, p1_symm])\n",
    "P3b = seg_intersect(l_10_symm_extension, l_00)\n",
    "l_10_symm_extension = np.array([symm_helper_y, P3b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_00 = np.array([u1, P1b])\n",
    "l_01 = np.array([u2, P1a])\n",
    "l_10 = np.array([u4, p4])\n",
    "l_11 = np.array([u3, P3a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot unit square\n",
    "plt.plot(lu1[:, 0], lu1[:, 1], \"r-\")\n",
    "plt.plot(lu2[:, 0], lu2[:, 1], \"r-\")\n",
    "plt.plot(lu3[:, 0], lu3[:, 1], \"r-\")\n",
    "plt.plot(lu4[:, 0], lu4[:, 1], \"r-\")\n",
    "\n",
    "# step 1: plot l_10 in contracting direction\n",
    "plt.plot(l_10[:, 0], l_10[:, 1], \"b-\")\n",
    "\n",
    "# step 2: plot l_00 and l_11 in expanding directions\n",
    "plt.plot(l_00[:, 0], l_00[:, 1], \"b-\")\n",
    "plt.plot(l_11[:, 0], l_11[:, 1], \"b-\")\n",
    "\n",
    "# step 3: plot l_01 in contracting direction \n",
    "plt.plot(l_01[:, 0], l_01[:, 1], \"b-\")\n",
    "\n",
    "# step 4: plot symmetric extension of l_01 line\n",
    "plt.plot(symm_helper_l11[:, 0], symm_helper_l11[:, 1], \"m--\")\n",
    "plt.plot(l_10_symm_extension[:, 0], l_10_symm_extension[:, 1], \"b-\")\n",
    "\n",
    "# plot intersection points\n",
    "plt.plot(P1a[0], P1a[1], \"bo\")\n",
    "plt.plot(P1b[0], P1b[1], \"ro\")\n",
    "plt.plot(P3a[0], P3a[1], \"go\")\n",
    "plt.plot(P3b[0], P3b[1], \"yo\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P1A = Polygon([P1a, u1, u2])\n",
    "P1B = Polygon([P3a, u3, u4])\n",
    "P1 = MultiPolygon([P1A, P1B])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P2A = Polygon([P3b, u1, symm_helper_l11[1]])\n",
    "P2B = Polygon([symm_helper_l11[0], u2, P1a, P1b])\n",
    "P2 = MultiPolygon([P2A, P2B])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P3A = Polygon([P1b, u4, symm_helper_l11[1], P3b])\n",
    "P3B = Polygon([symm_helper_l11[0], u3, P3a])\n",
    "P3 = MultiPolygon([P3A, P3B])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P1_vertices = [P1a, u1, u2, P3a]\n",
    "P2_vertices = [P3b, u1, P1a, P1b]\n",
    "P3_vertices = [P1b, u4, P3b, P3a]\n",
    "partition_vertices = [P1_vertices, P2_vertices, P3_vertices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = [P1, P2, P3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MultiPolygon([P1A, P1B, P2A, P2B, P3A, P3B])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P1A_phi = Polygon([P1b, u4, u1])\n",
    "P1B_phi = Polygon([u2, u3, phi(P3a)])\n",
    "P1_phi = MultiPolygon([P1A_phi, P1B_phi])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symm_helper_x = seg_intersect(lu3, l_00)\n",
    "symm_helper_y = q(symm_helper_x)\n",
    "symm_helper_l00 = np.array([symm_helper_x, symm_helper_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P2A_phi = Polygon([symm_helper_l00[1], phi(P1b), P1a, u1])\n",
    "P2B_phi = Polygon([symm_helper_l00[0], u4, P1b])\n",
    "P2_phi = MultiPolygon([P2A_phi, P2B_phi])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P3A_phi = Polygon([phi(P3a), P1a, symm_helper_l00[0], u3])\n",
    "P3B_phi = Polygon([u2, phi(P1b), symm_helper_l00[1]])\n",
    "P3_phi = MultiPolygon([P3A_phi, P3B_phi])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MultiPolygon([P1A_phi, P1B_phi, P2A_phi, P2B_phi, P3A_phi, P3B_phi])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_phi = [P1_phi, P2_phi, P3_phi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_non_identified(x: np.array) -> np.array:\n",
    "    return np.dot(A, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_phi(x: np.array) -> np.array:\n",
    "    return np.transpose(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_id = 1\n",
    "gamma = 0.8\n",
    "c = 3000\n",
    "tau = 0.0001\n",
    "target_state = np.array([0.5, 0.5])\n",
    "dynamic_system = DynamicSystem(phi_non_identified, d_phi, m_id)\n",
    "markov_decision_process = MarkovDecisionProcess(dynamic_system, partition, partition_vertices=partition_vertices, target_state=target_state, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add markov partition to overall experiment dictionary\n",
    "baseline_partitions[\"markov_1\"] = {}\n",
    "baseline_partitions[\"markov_1\"][\"partition\"] = partition\n",
    "baseline_partitions[\"markov_1\"][\"partition_vertices\"] = partition_vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Estimate transition probabilities\")\n",
    "start_time_estimation = time.time()\n",
    "_, num_iters_per_state = markov_decision_process.estimate_probability_matrix_pi_method(c=c, tau=tau, max_sample_trials=1000)\n",
    "total_time_estimation = time.time() - start_time_estimation\n",
    "print(f\"Transition probability estimation took {round(total_time_estimation, 2)}s\")\n",
    "print(f\"Number of iterates per state: {num_iters_per_state}\")\n",
    "\n",
    "print(\"Evaluate policy\")\n",
    "epsilon = 10e-6 # 2 * np.finfo(float).eps\n",
    "start_time_evaluation = time.time()\n",
    "V, num_iters, convergence_info = markov_decision_process.policy_evaluation(markov_decision_process.g, epsilon=epsilon)\n",
    "total_time_evaluation = time.time() - start_time_evaluation\n",
    "print(f\"Policy evaluation took {total_time_evaluation}s\")\n",
    "print(f\"Value function: {V}\")\n",
    "print(f\"Number of iterations until convergence: {num_iters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = list(range(1, num_iters))\n",
    "plt.plot(x_values, convergence_info[\"max_dist\"][1:])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"MAX-DIST\")\n",
    "plt.title(\"Maximal convergence distance over iterations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = list(range(1, num_iters))\n",
    "plt.plot(x_values, convergence_info[\"avg_dist\"][1:])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"L2-DIST\")\n",
    "plt.title(\"L2 convergence distance over iterations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build second baseline partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = u1 + (1/v1[0]) * v1\n",
    "p2 = u2 - (1/v2[1]) * v2\n",
    "p3 = u3 - (1/v1[0]) * v1\n",
    "p4 = u4 + (1/v2[1]) * v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_00 = np.array([u1, p1])\n",
    "l_01 = np.array([u2, p2])\n",
    "l_10 = np.array([u4, p4])\n",
    "l_11 = np.array([u3, p3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P1a = seg_intersect(l_00, l_01)\n",
    "P1b = seg_intersect(l_01, l_11)\n",
    "\n",
    "P3a = seg_intersect(l_10, l_00)\n",
    "\n",
    "symm_helper_x = seg_intersect(lu3, l_00)\n",
    "symm_helper_y = q(symm_helper_x)\n",
    "symm_helper_l00 = np.array([symm_helper_x, symm_helper_y])\n",
    "\n",
    "p1_symm = symm_helper_y + v1\n",
    "l_00_symm_extension = np.array([symm_helper_y, p1_symm])\n",
    "P3b = seg_intersect(l_00_symm_extension, l_01)\n",
    "l_00_symm_extension = np.array([symm_helper_y, P3b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_00 = np.array([u1, p1])\n",
    "l_01 = np.array([u2, P1a])\n",
    "l_10 = np.array([u4, P3a])\n",
    "l_11 = np.array([u3, P1b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot unit square\n",
    "plt.plot(lu1[:, 0], lu1[:, 1], \"r-\")\n",
    "plt.plot(lu2[:, 0], lu2[:, 1], \"r-\")\n",
    "plt.plot(lu3[:, 0], lu3[:, 1], \"r-\")\n",
    "plt.plot(lu4[:, 0], lu4[:, 1], \"r-\")\n",
    "\n",
    "# step 1: plot l_00 in expanding direction \n",
    "plt.plot(l_00[:, 0], l_00[:, 1], \"b-\")\n",
    "\n",
    "# step 2: plot l_01 and l_10 in contracting directions\n",
    "plt.plot(l_01[:, 0], l_01[:, 1], \"b-\")\n",
    "plt.plot(l_10[:, 0], l_10[:, 1], \"b-\")\n",
    "\n",
    "# step 3: plot l_11 in expanding direction\n",
    "plt.plot(l_11[:, 0], l_11[:, 1], \"b-\")\n",
    "\n",
    "# step 4: plot symmetric extension of l_00 line\n",
    "plt.plot(symm_helper_l00[:, 0], symm_helper_l00[:, 1], \"m--\")\n",
    "plt.plot(l_00_symm_extension[:, 0], l_00_symm_extension[:, 1], \"b-\")\n",
    "\n",
    "# plot intersection points\n",
    "plt.plot(P1a[0], P1a[1], \"bo\")\n",
    "plt.plot(P1b[0], P1b[1], \"ro\")\n",
    "plt.plot(P3a[0], P3a[1], \"go\")\n",
    "plt.plot(P3b[0], P3b[1], \"yo\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P1A = Polygon([P3b, symm_helper_l00[1], u1, P1a])\n",
    "P1B = Polygon([symm_helper_l00[0], u4, P3a])\n",
    "P1 = MultiPolygon([P1A, P1B])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P2A = Polygon([P1b, u3, symm_helper_l00[0], P1a])\n",
    "P2B = Polygon([symm_helper_l00[1], u2, P3b])\n",
    "P2 = MultiPolygon([P2A, P2B])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P3A = Polygon([P1b, u2, u3])\n",
    "P3B = Polygon([u1, P3a, u4])\n",
    "P3 = MultiPolygon([P3A, P3B])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P1_vertices = [u1, P1a, P3b, P3a]\n",
    "P2_vertices = [P1b, P1a, P3b, u3]\n",
    "P3_vertices = [P1b, u3, u2, P3a]\n",
    "partition_vertices = [P1_vertices, P2_vertices, P3_vertices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = [P1, P2, P3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MultiPolygon([P1A, P1B, P2A, P2B, P3A, P3B])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add markov partition to overall experiment dictionary\n",
    "baseline_partitions[\"markov_2\"] = {}\n",
    "baseline_partitions[\"markov_2\"][\"partition\"] = partition\n",
    "baseline_partitions[\"markov_2\"][\"partition_vertices\"] = partition_vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"results/baseline_3_partitions.pkl\", \"wb\") as file:\n",
    "    pickle.dump(baseline_partitions, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test policy evaluation for a baseline partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_baseline_partition = baseline_partitions[\"horizontal\"][\"partition\"]\n",
    "test_baseline_partition_vertices = baseline_partitions[\"horizontal\"][\"partition_vertices\"]\n",
    "markov_decision_process = MarkovDecisionProcess(dynamic_system, test_baseline_partition, partition_vertices=test_baseline_partition_vertices, target_state=target_state, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Estimate transition probabilities\")\n",
    "start_time_estimation = time.time()\n",
    "_, num_iters_per_state = markov_decision_process.estimate_probability_matrix_pi_method(c=c, tau=tau, max_sample_trials=1000)\n",
    "total_time_estimation = time.time() - start_time_estimation\n",
    "print(f\"Transition probability estimation took {round(total_time_estimation, 2)}s\")\n",
    "print(f\"Number of iterates per state: {num_iters_per_state}\")\n",
    "\n",
    "print(\"Evaluate policy\")\n",
    "epsilon = 10e-6 # 2 * np.finfo(float).eps\n",
    "start_time_evaluation = time.time()\n",
    "V, num_iters, convergence_info = markov_decision_process.policy_evaluation(markov_decision_process.g, epsilon=epsilon)\n",
    "total_time_evaluation = time.time() - start_time_evaluation\n",
    "print(f\"Policy evaluation took {total_time_evaluation}s\")\n",
    "print(f\"Value function: {V}\")\n",
    "print(f\"Number of iterations until convergence: {num_iters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = list(range(1, num_iters))\n",
    "plt.plot(x_values, convergence_info[\"max_dist\"][1:])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"MAX-DIST\")\n",
    "plt.title(\"Maximal convergence distance over iterations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = list(range(1, num_iters))\n",
    "plt.plot(x_values, convergence_info[\"avg_dist\"][1:])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"L2-DIST\")\n",
    "plt.title(\"L2 convergence distance over iterations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with baseline and markov partitions of 3 subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_candidates = np.linspace(start=0.05, stop=0.95, num=2)\n",
    "estimation_repetitions = 1\n",
    "experiment_results = {}\n",
    "\n",
    "for experiment_run in baseline_partitions.keys():\n",
    "    print(\"\\n---------------------------------------\\n\")\n",
    "    print(f\"Experiment run with {experiment_run} partition\")\n",
    "\n",
    "    partition = baseline_partitions[experiment_run][\"partition\"]\n",
    "    partition_vertices = baseline_partitions[experiment_run][\"partition_vertices\"]\n",
    "    markov_decision_process = MarkovDecisionProcess(dynamic_system, partition, partition_vertices=partition_vertices, target_state=target_state)\n",
    "\n",
    "    print(\"Estimate transition probabilities\")\n",
    "    n = len(partition)\n",
    "    total_time_estimation_results = np.zeros((estimation_repetitions, n))\n",
    "    num_iters_per_state_results = np.zeros((estimation_repetitions, n))\n",
    "\n",
    "    for i in range(estimation_repetitions): \n",
    "        start_time_estimation = time.time()\n",
    "        _, num_iters_per_state = markov_decision_process.estimate_probability_matrix_pi_method(c=c, tau=tau, max_sample_trials=1000)\n",
    "        total_time_estimation = time.time() - start_time_estimation\n",
    "        total_time_estimation_results[i, :] = total_time_estimation\n",
    "        num_iters_per_state_results[i, :] = num_iters_per_state\n",
    "\n",
    "    print(f\"Transition probability estimation took {round(np.mean(total_time_estimation_results), 2)}s on average with {round(np.var(total_time_estimation_results), 2)}s variance.\")\n",
    "    print(f\"Number of iterates per state {np.mean(num_iters_per_state_results, axis=0)} on average with variance {np.var(num_iters_per_state_results, axis=0)}\")\n",
    "\n",
    "    experiment_results[experiment_run] = {}\n",
    "    experiment_results[experiment_run][\"partition\"] = partition\n",
    "    experiment_results[experiment_run][\"partition_vertices\"] = partition_vertices\n",
    "    experiment_results[experiment_run][\"estimation_run_time\"] = total_time_estimation_results\n",
    "    experiment_results[experiment_run][\"num_iters_per_state\"] = num_iters_per_state_results\n",
    "    experiment_results[experiment_run][\"policy_evaluation\"] = {}\n",
    "\n",
    "    for gamma in gamma_candidates:\n",
    "        print(f\"\\nEvaluate policy with gamma={gamma}\")\n",
    "        experiment_results[experiment_run][\"policy_evaluation\"][gamma] = {}\n",
    "        markov_decision_process.gamma = gamma\n",
    "\n",
    "        epsilon = 10e-6 # 2 * np.finfo(float).eps\n",
    "        start_time_evaluation = time.time()\n",
    "        V, num_iters, convergence_info = markov_decision_process.policy_evaluation(markov_decision_process.g, epsilon=epsilon)\n",
    "        total_time_evaluation = time.time() - start_time_evaluation\n",
    "        print(f\"Policy evaluation took {total_time_evaluation}s\")\n",
    "        print(f\"Value function: {V}\")\n",
    "        print(f\"Number of iterations until convergence: {num_iters}\")\n",
    "\n",
    "        experiment_results[experiment_run][\"policy_evaluation\"][gamma][\"run_time\"] = total_time_evaluation\n",
    "        experiment_results[experiment_run][\"policy_evaluation\"][gamma][\"value_function\"] = V\n",
    "        experiment_results[experiment_run][\"policy_evaluation\"][gamma][\"num_iters\"] = num_iters\n",
    "        experiment_results[experiment_run][\"policy_evaluation\"][gamma][\"convergence_info\"] = convergence_info\n",
    "\n",
    "with open(f\"results/policy_evaluation_3_partition_results.pkl\", \"wb\") as file:\n",
    "    pickle.dump(experiment_results, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize experiment results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/policy_evaluation_3_partition_results.pkl\", \"rb\") as file:\n",
    "    experiment_results = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment_run in experiment_results.keys():\n",
    "    print(\"\\n---------------------------------------\\n\")\n",
    "    print(f\"Experiment run with {experiment_run} partition\")\n",
    "\n",
    "    for gamma in experiment_results[experiment_run][\"policy_evaluation\"]:\n",
    "        num_iters = experiment_results[experiment_run][\"policy_evaluation\"][gamma][\"num_iters\"]\n",
    "        convergence_info = experiment_results[experiment_run][\"policy_evaluation\"][gamma][\"convergence_info\"]\n",
    "        x_values = list(range(1, num_iters))\n",
    "\n",
    "        plt.plot(x_values, convergence_info[\"max_dist\"][1:])\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.ylabel(\"MAX-DIST\")\n",
    "        plt.title(f\"Maximal convergence distance for gamma={round(gamma, 2)}\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(x_values, convergence_info[\"avg_dist\"][1:])\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.ylabel(\"L2-DIST\")\n",
    "        plt.title(f\"L2 convergence distance for gamma={round(gamma, 2)}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimation_convergence_results = []\n",
    "for experiment_run in experiment_results.keys():\n",
    "    estimation_run_time_avg = np.mean(experiment_results[experiment_run][\"estimation_run_time\"])\n",
    "    estimation_run_time_var = np.var(experiment_results[experiment_run][\"estimation_run_time\"])\n",
    "\n",
    "    sum_estimation_iters_avg = np.mean(np.sum(experiment_results[experiment_run][\"num_iters_per_state\"], axis=1))\n",
    "    sum_estimation_iters_var = np.var(np.sum(experiment_results[experiment_run][\"num_iters_per_state\"], axis=1))\n",
    "\n",
    "    avg_estimation_iters_avg = np.mean(np.mean(experiment_results[experiment_run][\"num_iters_per_state\"], axis=1))\n",
    "    avg_estimation_iters_var = np.var(np.mean(experiment_results[experiment_run][\"num_iters_per_state\"], axis=1))\n",
    "\n",
    "    max_estimation_iters_avg = np.mean(np.max(experiment_results[experiment_run][\"num_iters_per_state\"], axis=1))\n",
    "    max_estimation_iters_var = np.var(np.max(experiment_results[experiment_run][\"num_iters_per_state\"], axis=1))\n",
    "\n",
    "    estimation_convergence_result = {\n",
    "        \"partition_method\": experiment_run,\n",
    "        \"estimation_run_time_avg\": estimation_run_time_avg,\n",
    "        \"estimation_run_time_var\": estimation_run_time_var,\n",
    "        \"sum_estimation_iters_avg\": sum_estimation_iters_avg,\n",
    "        \"sum_estimation_iters_var\": sum_estimation_iters_var,\n",
    "        \"avg_estimation_iters_avg\": avg_estimation_iters_avg,\n",
    "        \"avg_estimation_iters_var\": avg_estimation_iters_var,\n",
    "        \"max_estimation_iters_avg\": max_estimation_iters_avg,\n",
    "        \"max_estimation_iters_var\": max_estimation_iters_var,\n",
    "    }\n",
    "    \n",
    "    estimation_convergence_results.append(estimation_convergence_result)\n",
    "\n",
    "estimation_convergence_results_df = pd.DataFrame(estimation_convergence_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimation_convergence_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_gamma = 0.95\n",
    "num_iters_list = []\n",
    "\n",
    "for experiment_run in experiment_results.keys():\n",
    "    num_iters = experiment_results[experiment_run][\"policy_evaluation\"][fixed_gamma][\"num_iters\"]\n",
    "    num_iters_dict = {\"partition\": experiment_run, \"num_iters\": num_iters}\n",
    "    num_iters_list.append(num_iters_dict)\n",
    "\n",
    "num_iters_df = pd.DataFrame(num_iters_list)\n",
    "\n",
    "ax = sns.barplot(x=\"partition\", y=\"num_iters\", data=num_iters_df, hue=\"partition\", saturation=8, dodge=False)\n",
    "ax.set(xlabel=\"partition method\", ylabel=\"Number of Iterations\")\n",
    "plt.title(\"Policy Evaluation Convergence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_cols = [\"partition_method\", \"sum_estimation_iters_avg\", \"avg_estimation_iters_avg\", \"max_estimation_iters_avg\"] \n",
    "cols_naming = [\"Partition\", \"Sum of Iters\", \"Average Iters\", \"Max Iters\"]\n",
    "latex_table_str = estimation_convergence_results_df.to_latex(index=False, columns=relevant_cols, header=cols_naming, float_format=\"%.2f\", bold_rows=True)\n",
    "\n",
    "with open(\"results/latex_table_3_partitions_export.txt\", \"w\") as file:\n",
    "    file.write(latex_table_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iters_list = []\n",
    "\n",
    "for experiment_run in experiment_results.keys():\n",
    "    for gamma in experiment_results[experiment_run][\"policy_evaluation\"]:\n",
    "        num_iters = experiment_results[experiment_run][\"policy_evaluation\"][gamma][\"num_iters\"]\n",
    "        num_iters_dict = {\"partition\": experiment_run, \"gamma\": gamma, \"num_iters\": num_iters}\n",
    "        num_iters_list.append(num_iters_dict)\n",
    "\n",
    "num_iters_df = pd.DataFrame(num_iters_list)\n",
    "\n",
    "ax = sns.lineplot(data=num_iters_df, x=\"gamma\", y=\"num_iters\", hue=\"partition\")\n",
    "ax.set(xlabel=\"discount factor gamma\", ylabel=\"Number of Iterations\")\n",
    "plt.title(\"Policy Evaluation Convergence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Markov partition with 7 subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load baseline partitions from disk, if they already exist\n",
    "if os.path.isfile(\"results/baseline_7_partitions.pkl\"):\n",
    "    with open(\"results/baseline_7_partitions.pkl\", \"rb\") as file:\n",
    "        baseline_partitions = pickle.load(file)\n",
    "else:\n",
    "    baseline_partitions = {}\n",
    "\n",
    "baseline_partitions.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_partitions = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1,1], [1,0]])\n",
    "default_delta = 10e-3\n",
    "num_iters = 2\n",
    "m_id = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_non_identified(x: np.array) -> np.array:\n",
    "    return np.dot(A, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_phi(x: np.array) -> np.array:\n",
    "    return np.transpose(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_system = DynamicSystem(phi_non_identified, d_phi, m_id)\n",
    "partition = Partition(dynamic_system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "branches, intersection_points = partition.compute_partition(num_iters, delta=default_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition.plot_partition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices = intersection_points.copy()\n",
    "vertices.extend([u1, u2, u3, u4])\n",
    "vertices = np.array(vertices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P1_vertices = vertices[[0,1,3,2]]\n",
    "P2_vertices = vertices[[0,2,5,8]]\n",
    "P3_vertices = vertices[[1,3,6,10]]\n",
    "P4_vertices = vertices[[2,3,4,11]]\n",
    "P5_vertices = vertices[[0,1,7,8]]\n",
    "P6_vertices = vertices[[0,3,5,6]]\n",
    "P7_vertices = vertices[[1,2,4,7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_vertices = [P1_vertices, P2_vertices, P3_vertices, P4_vertices, P5_vertices, P6_vertices, P7_vertices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = symm_helper_l11[0,0]\n",
    "c2 = symm_helper_l00[0,1]\n",
    "\n",
    "x1 = np.array([c1, 0])\n",
    "x2 = np.array([c2, 0])\n",
    "x3 = np.array([c1, 1])\n",
    "x4 = np.array([c2, 1])\n",
    "\n",
    "y1 = np.array([0, c1])\n",
    "y2 = np.array([0, c2])\n",
    "y3 = np.array([1, c1])\n",
    "y4 = np.array([1, c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P1A = Polygon(P1_vertices)\n",
    "P1 = MultiPolygon([P1A])\n",
    "\n",
    "P2A = Polygon([vertices[0], vertices[2], u1, y1])\n",
    "P2B = Polygon([u4, y3, vertices[5]])\n",
    "P2 = MultiPolygon([P2A, P2B])\n",
    "\n",
    "P3A = Polygon([vertices[1], vertices[3], y4, u3])\n",
    "P3B = Polygon([u2, y2, vertices[6]])\n",
    "P3 = MultiPolygon([P3A, P3B])\n",
    "\n",
    "P4A = Polygon([vertices[3], vertices[2], x2, u4])\n",
    "P4B = Polygon([u3, x4, vertices[4]])\n",
    "P4 = MultiPolygon([P4A, P4B])\n",
    "\n",
    "P5A = Polygon([vertices[0], vertices[1], x3, u2])\n",
    "P5B = Polygon([u1, x1, vertices[7]])\n",
    "P5 = MultiPolygon([P5A, P5B])\n",
    "\n",
    "P6A = Polygon([vertices[0], y1, y2, vertices[6]])\n",
    "P6B = Polygon([y3, y4, vertices[3], vertices[5]])\n",
    "P6 = MultiPolygon([P6A, P6B])\n",
    "\n",
    "P7A = Polygon([x3, x4, vertices[4], vertices[1]])\n",
    "P7B = Polygon([x1, x2, vertices[2], vertices[7]])\n",
    "P7 = MultiPolygon([P7A, P7B])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MultiPolygon([P1A, P2A, P2B, P3A, P3B, P4A, P4B, P5A, P5B, P6A, P6B, P7A, P7B])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = [P1, P2, P3, P4, P5, P6, P7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add markov partition to overall experiment dictionary\n",
    "baseline_partitions[\"markov_1\"] = {}\n",
    "baseline_partitions[\"markov_1\"][\"partition\"] = partition\n",
    "baseline_partitions[\"markov_1\"][\"partition_vertices\"] = partition_vertices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build baseline partitions with 7 subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 7\n",
    "y_offset = np.array([0, m_id / n])\n",
    "partition = []\n",
    "partition_vertices = []\n",
    "polygons = []\n",
    "\n",
    "for i in range(n):\n",
    "    p1 = u1 + i*y_offset\n",
    "    p2 = u1 + (i+1)*y_offset\n",
    "    p3 = u4 + (i+1)*y_offset\n",
    "    p4 = u4 + i*y_offset\n",
    "\n",
    "    P = MultiPolygon([Polygon([p1, p2, p3, p4])])\n",
    "    polygons.append(Polygon([p1, p2, p3, p4]))\n",
    "    partition.append(P)\n",
    "    partition_vertices. append([p1, p2, p3, p4])\n",
    "\n",
    "# add markov partition to overall experiment dictionary\n",
    "baseline_partitions[\"horizontal\"] = {}\n",
    "baseline_partitions[\"horizontal\"][\"partition\"] = partition\n",
    "baseline_partitions[\"horizontal\"][\"partition_vertices\"] = partition_vertices\n",
    "\n",
    "MultiPolygon(polygons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 7\n",
    "x_offset = np.array([m_id / n, 0])\n",
    "partition = []\n",
    "partition_vertices = []\n",
    "polygons = []\n",
    "\n",
    "for i in range(n):\n",
    "    p1 = u1 + i*x_offset\n",
    "    p2 = u1 + (i+1)*x_offset\n",
    "    p3 = u2 + (i+1)*x_offset\n",
    "    p4 = u2 + i*x_offset\n",
    "\n",
    "    P = MultiPolygon([Polygon([p1, p2, p3, p4])])\n",
    "    polygons.append(Polygon([p1, p2, p3, p4]))\n",
    "    partition.append(P)\n",
    "    partition_vertices. append([p1, p2, p3, p4])\n",
    "\n",
    "# add markov partition to overall experiment dictionary\n",
    "baseline_partitions[\"vertical\"] = {}\n",
    "baseline_partitions[\"vertical\"][\"partition\"] = partition\n",
    "baseline_partitions[\"vertical\"][\"partition_vertices\"] = partition_vertices\n",
    "\n",
    "MultiPolygon(polygons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = 2\n",
    "n2 = 3\n",
    "x_offset = np.array([m_id / n1, 0])\n",
    "y_offset = np.array([0, m_id / n2])\n",
    "\n",
    "partition = []\n",
    "partition_vertices = []\n",
    "polygons = []\n",
    "\n",
    "for i in range(n1):\n",
    "    for j in range(n2):\n",
    "        p1 = u1 + i*x_offset + j*y_offset\n",
    "        p2 = p1 + x_offset\n",
    "        p3 = p2 + y_offset\n",
    "        p4 = p3 - x_offset\n",
    "\n",
    "        P = MultiPolygon([Polygon([p1, p2, p3, p4])])\n",
    "        polygons.append(Polygon([p1, p2, p3, p4]))\n",
    "        partition.append(P)\n",
    "        partition_vertices. append([p1, p2, p3, p4])\n",
    "\n",
    "# add markov partition to overall experiment dictionary\n",
    "baseline_partitions[\"complex_1\"] = {}\n",
    "baseline_partitions[\"complex_1\"][\"partition\"] = partition\n",
    "baseline_partitions[\"complex_1\"][\"partition_vertices\"] = partition_vertices\n",
    "\n",
    "MultiPolygon(polygons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = 3\n",
    "n2 = 2\n",
    "x_offset = np.array([m_id / n1, 0])\n",
    "y_offset = np.array([0, m_id / n2])\n",
    "\n",
    "partition = []\n",
    "partition_vertices = []\n",
    "polygons = []\n",
    "\n",
    "for i in range(n1):\n",
    "    for j in range(n2):\n",
    "        p1 = u1 + i*x_offset + j*y_offset\n",
    "        p2 = p1 + x_offset\n",
    "        p3 = p2 + y_offset\n",
    "        p4 = p3 - x_offset\n",
    "\n",
    "        P = MultiPolygon([Polygon([p1, p2, p3, p4])])\n",
    "        polygons.append(Polygon([p1, p2, p3, p4]))\n",
    "        partition.append(P)\n",
    "        partition_vertices. append([p1, p2, p3, p4])\n",
    "\n",
    "# add markov partition to overall experiment dictionary\n",
    "baseline_partitions[\"complex_2\"] = {}\n",
    "baseline_partitions[\"complex_2\"][\"partition\"] = partition\n",
    "baseline_partitions[\"complex_2\"][\"partition_vertices\"] = partition_vertices\n",
    "\n",
    "MultiPolygon(polygons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"results/baseline_7_partitions.pkl\", \"wb\") as file:\n",
    "    pickle.dump(baseline_partitions, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with all 7 subset partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_candidates = np.linspace(start=0.05, stop=0.95, num=2)\n",
    "estimation_repetitions = 1\n",
    "experiment_results = {}\n",
    "\n",
    "for experiment_run in baseline_partitions.keys():\n",
    "    print(\"\\n---------------------------------------\\n\")\n",
    "    print(f\"Experiment run with {experiment_run} partition\")\n",
    "\n",
    "    partition = baseline_partitions[experiment_run][\"partition\"]\n",
    "    partition_vertices = baseline_partitions[experiment_run][\"partition_vertices\"]\n",
    "    markov_decision_process = MarkovDecisionProcess(dynamic_system, partition, partition_vertices=partition_vertices, target_state=target_state)\n",
    "\n",
    "    print(\"Estimate transition probabilities\")\n",
    "    n = len(partition)\n",
    "    total_time_estimation_results = np.zeros((estimation_repetitions, n))\n",
    "    num_iters_per_state_results = np.zeros((estimation_repetitions, n))\n",
    "\n",
    "    for i in range(estimation_repetitions): \n",
    "        start_time_estimation = time.time()\n",
    "        _, num_iters_per_state = markov_decision_process.estimate_probability_matrix_pi_method(c=c, tau=tau, max_sample_trials=1000)\n",
    "        total_time_estimation = time.time() - start_time_estimation\n",
    "        total_time_estimation_results[i, :] = total_time_estimation\n",
    "        num_iters_per_state_results[i, :] = num_iters_per_state\n",
    "\n",
    "    print(f\"Transition probability estimation took {round(np.mean(total_time_estimation_results), 2)}s on average with {round(np.var(total_time_estimation_results), 2)}s variance.\")\n",
    "    print(f\"Number of iterates per state {np.mean(num_iters_per_state_results, axis=0)} on average with variance {np.var(num_iters_per_state_results, axis=0)}\")\n",
    "\n",
    "    experiment_results[experiment_run] = {}\n",
    "    experiment_results[experiment_run][\"partition\"] = partition\n",
    "    experiment_results[experiment_run][\"partition_vertices\"] = partition_vertices\n",
    "    experiment_results[experiment_run][\"estimation_run_time\"] = total_time_estimation_results\n",
    "    experiment_results[experiment_run][\"num_iters_per_state\"] = num_iters_per_state_results\n",
    "    experiment_results[experiment_run][\"policy_evaluation\"] = {}\n",
    "\n",
    "    for gamma in gamma_candidates:\n",
    "        print(f\"\\nEvaluate policy with gamma={gamma}\")\n",
    "        experiment_results[experiment_run][\"policy_evaluation\"][gamma] = {}\n",
    "        markov_decision_process.gamma = gamma\n",
    "\n",
    "        epsilon = 10e-6 # 2 * np.finfo(float).eps\n",
    "        start_time_evaluation = time.time()\n",
    "        V, num_iters, convergence_info = markov_decision_process.policy_evaluation(markov_decision_process.g, epsilon=epsilon)\n",
    "        total_time_evaluation = time.time() - start_time_evaluation\n",
    "        print(f\"Policy evaluation took {total_time_evaluation}s\")\n",
    "        print(f\"Value function: {V}\")\n",
    "        print(f\"Number of iterations until convergence: {num_iters}\")\n",
    "\n",
    "        experiment_results[experiment_run][\"policy_evaluation\"][gamma][\"run_time\"] = total_time_evaluation\n",
    "        experiment_results[experiment_run][\"policy_evaluation\"][gamma][\"value_function\"] = V\n",
    "        experiment_results[experiment_run][\"policy_evaluation\"][gamma][\"num_iters\"] = num_iters\n",
    "        experiment_results[experiment_run][\"policy_evaluation\"][gamma][\"convergence_info\"] = convergence_info\n",
    "\n",
    "with open(f\"results/policy_evaluation_7_partition_results.pkl\", \"wb\") as file:\n",
    "    pickle.dump(experiment_results, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize experiment results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/policy_evaluation_7_partition_results.pkl\", \"rb\") as file:\n",
    "    experiment_results = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment_run in experiment_results.keys():\n",
    "    print(\"\\n---------------------------------------\\n\")\n",
    "    print(f\"Experiment run with {experiment_run} partition\")\n",
    "\n",
    "    for gamma in experiment_results[experiment_run][\"policy_evaluation\"]:\n",
    "        num_iters = experiment_results[experiment_run][\"policy_evaluation\"][gamma][\"num_iters\"]\n",
    "        convergence_info = experiment_results[experiment_run][\"policy_evaluation\"][gamma][\"convergence_info\"]\n",
    "        x_values = list(range(1, num_iters))\n",
    "\n",
    "        plt.plot(x_values, convergence_info[\"max_dist\"][1:])\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.ylabel(\"MAX-DIST\")\n",
    "        plt.title(f\"Maximal convergence distance for gamma={round(gamma, 2)}\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(x_values, convergence_info[\"avg_dist\"][1:])\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.ylabel(\"L2-DIST\")\n",
    "        plt.title(f\"L2 convergence distance for gamma={round(gamma, 2)}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimation_convergence_results = []\n",
    "for experiment_run in experiment_results.keys():\n",
    "    estimation_run_time_avg = np.mean(experiment_results[experiment_run][\"estimation_run_time\"])\n",
    "    estimation_run_time_var = np.var(experiment_results[experiment_run][\"estimation_run_time\"])\n",
    "\n",
    "    sum_estimation_iters_avg = np.mean(np.sum(experiment_results[experiment_run][\"num_iters_per_state\"], axis=1))\n",
    "    sum_estimation_iters_var = np.var(np.sum(experiment_results[experiment_run][\"num_iters_per_state\"], axis=1))\n",
    "\n",
    "    avg_estimation_iters_avg = np.mean(np.mean(experiment_results[experiment_run][\"num_iters_per_state\"], axis=1))\n",
    "    avg_estimation_iters_var = np.var(np.mean(experiment_results[experiment_run][\"num_iters_per_state\"], axis=1))\n",
    "\n",
    "    max_estimation_iters_avg = np.mean(np.max(experiment_results[experiment_run][\"num_iters_per_state\"], axis=1))\n",
    "    max_estimation_iters_var = np.var(np.max(experiment_results[experiment_run][\"num_iters_per_state\"], axis=1))\n",
    "\n",
    "    estimation_convergence_result = {\n",
    "        \"partition_method\": experiment_run,\n",
    "        \"estimation_run_time_avg\": estimation_run_time_avg,\n",
    "        \"estimation_run_time_var\": estimation_run_time_var,\n",
    "        \"sum_estimation_iters_avg\": sum_estimation_iters_avg,\n",
    "        \"sum_estimation_iters_var\": sum_estimation_iters_var,\n",
    "        \"avg_estimation_iters_avg\": avg_estimation_iters_avg,\n",
    "        \"avg_estimation_iters_var\": avg_estimation_iters_var,\n",
    "        \"max_estimation_iters_avg\": max_estimation_iters_avg,\n",
    "        \"max_estimation_iters_var\": max_estimation_iters_var,\n",
    "    }\n",
    "    \n",
    "    estimation_convergence_results.append(estimation_convergence_result)\n",
    "\n",
    "estimation_convergence_results_df = pd.DataFrame(estimation_convergence_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimation_convergence_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_cols = [\"partition_method\", \"sum_estimation_iters_avg\", \"avg_estimation_iters_avg\", \"max_estimation_iters_avg\"] \n",
    "cols_naming = [\"Partition\", \"Sum of Iters\", \"Average Iters\", \"Max Iters\"]\n",
    "latex_table_str = estimation_convergence_results_df.to_latex(index=False, columns=relevant_cols, header=cols_naming, float_format=\"%.2f\", bold_rows=True)\n",
    "\n",
    "with open(\"results/latex_table_7_partitions_export.txt\", \"w\") as file:\n",
    "    file.write(latex_table_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_gamma = 0.95\n",
    "num_iters_list = []\n",
    "\n",
    "for experiment_run in experiment_results.keys():\n",
    "    num_iters = experiment_results[experiment_run][\"policy_evaluation\"][fixed_gamma][\"num_iters\"]\n",
    "    num_iters_dict = {\"partition\": experiment_run, \"num_iters\": num_iters}\n",
    "    num_iters_list.append(num_iters_dict)\n",
    "\n",
    "num_iters_df = pd.DataFrame(num_iters_list)\n",
    "\n",
    "ax = sns.barplot(x=\"partition\", y=\"num_iters\", data=num_iters_df, hue=\"partition\", saturation=8, dodge=False)\n",
    "ax.set(xlabel=\"partition method\", ylabel=\"Number of Iterations\")\n",
    "plt.title(\"Policy Evaluation Convergence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iters_list = []\n",
    "\n",
    "for experiment_run in experiment_results.keys():\n",
    "    for gamma in experiment_results[experiment_run][\"policy_evaluation\"]:\n",
    "        num_iters = experiment_results[experiment_run][\"policy_evaluation\"][gamma][\"num_iters\"]\n",
    "        num_iters_dict = {\"partition\": experiment_run, \"gamma\": gamma, \"num_iters\": num_iters}\n",
    "        num_iters_list.append(num_iters_dict)\n",
    "\n",
    "num_iters_df = pd.DataFrame(num_iters_list)\n",
    "\n",
    "ax = sns.lineplot(data=num_iters_df, x=\"gamma\", y=\"num_iters\", hue=\"partition\")\n",
    "ax.set(xlabel=\"discount factor gamma\", ylabel=\"Number of Iterations\")\n",
    "plt.title(\"Policy Evaluation Convergence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Markov partition with 11 subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load baseline partitions from disk, if they already exist\n",
    "if os.path.isfile(\"results/baseline_11_partitions.pkl\"):\n",
    "    with open(\"results/baseline_11_partitions.pkl\", \"rb\") as file:\n",
    "        baseline_partitions = pickle.load(file)\n",
    "else:\n",
    "    baseline_partitions = {}\n",
    "\n",
    "baseline_partitions.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_partitions = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1,1], [1,0]])\n",
    "default_delta = 10e-3\n",
    "num_iters = 3\n",
    "m_id = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_non_identified(x: np.array) -> np.array:\n",
    "    return np.dot(A, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_phi(x: np.array) -> np.array:\n",
    "    return np.transpose(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_system = DynamicSystem(phi_non_identified, d_phi, m_id)\n",
    "partition = Partition(dynamic_system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "branches, intersection_points = partition.compute_partition(num_iters, delta=default_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition.plot_partition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices = intersection_points.copy()\n",
    "vertices.extend([u1, u2, u3, u4])\n",
    "vertices = np.array(vertices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P1_vertices = vertices[[0,1,3,2]]\n",
    "P2_vertices = vertices[[0,2,7,9]]\n",
    "P3_vertices = vertices[[0,1,11,6]]\n",
    "P4_vertices = vertices[[1,3,10,4]]\n",
    "P5_vertices = vertices[[3,2,8,5]]\n",
    "P6_vertices = vertices[[3,5,0,6]]\n",
    "P7_vertices = vertices[[2,7,1,4]]\n",
    "P8_vertices = vertices[[8,5,15,4]]\n",
    "P9_vertices = vertices[[4,14,6,10]]\n",
    "P10_vertices = vertices[[7,12,6,11]]\n",
    "P11_vertices = vertices[[7,12,5,9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_vertices = [P1_vertices, P2_vertices, P3_vertices, P4_vertices, P5_vertices, P6_vertices, P7_vertices, P8_vertices, P9_vertices, P10_vertices, P11_vertices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P1A = Polygon(P1_vertices)\n",
    "P1 = MultiPolygon([P1A])\n",
    "\n",
    "P2A = Polygon(P2_vertices)\n",
    "P2 = MultiPolygon([P2A])\n",
    "\n",
    "P3A = Polygon(P3_vertices)\n",
    "P3 = MultiPolygon([P3A])\n",
    "\n",
    "P4A = Polygon(P4_vertices)\n",
    "P4 = MultiPolygon([P4A])\n",
    "\n",
    "P5A = Polygon(P5_vertices)\n",
    "P5 = MultiPolygon([P5A])\n",
    "\n",
    "P6A = Polygon([vertices[3], vertices[5], y3, y4])\n",
    "P6B = Polygon([y1, y2, vertices[6], vertices[0]])\n",
    "P6 = MultiPolygon([P6A, P6B])\n",
    "\n",
    "P7A = Polygon([vertices[2], vertices[7], x1, x2])\n",
    "P7B = Polygon([x3, x4, vertices[4], vertices[1]])\n",
    "P7 = MultiPolygon([P7A, P7B])\n",
    "\n",
    "P8A = Polygon([vertices[8], vertices[5], vertices[15], x2])\n",
    "P8B = Polygon([u3, x4, vertices[4]])\n",
    "P8 = MultiPolygon([P8A, P8B])\n",
    "\n",
    "P9A = Polygon([vertices[4], u3, y4, vertices[10]])\n",
    "P9B = Polygon([u2, y2, vertices[6]])\n",
    "P9 = MultiPolygon([P9A, P9B])\n",
    "\n",
    "P10A = Polygon([u1, vertices[7], x1])\n",
    "P10B = Polygon([u2, x3, vertices[11], vertices[6]])\n",
    "P10 = MultiPolygon([P10A, P10B])\n",
    "\n",
    "P11A = Polygon([u1, y1, vertices[9], vertices[7]])\n",
    "P11B = Polygon([u4, y3, vertices[5]])\n",
    "P11 = MultiPolygon([P11A, P11B])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MultiPolygon([P1A, P2A, P3A, P4A, P5A, P6A, P6B, P7A, P7B, P8A, P8B, P9A, P9B, P10A, P10B, P11A, P11B])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = [P1, P2, P3, P4, P5, P6, P7, P8, P9, P10, P11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add markov partition to overall experiment dictionary\n",
    "baseline_partitions[\"markov_1\"] = {}\n",
    "baseline_partitions[\"markov_1\"][\"partition\"] = partition\n",
    "baseline_partitions[\"markov_1\"][\"partition_vertices\"] = partition_vertices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build baseline partitions with 11 subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 11\n",
    "y_offset = np.array([0, m_id / n])\n",
    "partition = []\n",
    "partition_vertices = []\n",
    "polygons = []\n",
    "\n",
    "for i in range(n):\n",
    "    p1 = u1 + i*y_offset\n",
    "    p2 = u1 + (i+1)*y_offset\n",
    "    p3 = u4 + (i+1)*y_offset\n",
    "    p4 = u4 + i*y_offset\n",
    "\n",
    "    P = MultiPolygon([Polygon([p1, p2, p3, p4])])\n",
    "    polygons.append(Polygon([p1, p2, p3, p4]))\n",
    "    partition.append(P)\n",
    "    partition_vertices. append([p1, p2, p3, p4])\n",
    "\n",
    "# add markov partition to overall experiment dictionary\n",
    "baseline_partitions[\"horizontal\"] = {}\n",
    "baseline_partitions[\"horizontal\"][\"partition\"] = partition\n",
    "baseline_partitions[\"horizontal\"][\"partition_vertices\"] = partition_vertices\n",
    "\n",
    "MultiPolygon(polygons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 11\n",
    "x_offset = np.array([m_id / n, 0])\n",
    "partition = []\n",
    "partition_vertices = []\n",
    "polygons = []\n",
    "\n",
    "for i in range(n):\n",
    "    p1 = u1 + i*x_offset\n",
    "    p2 = u1 + (i+1)*x_offset\n",
    "    p3 = u2 + (i+1)*x_offset\n",
    "    p4 = u2 + i*x_offset\n",
    "\n",
    "    P = MultiPolygon([Polygon([p1, p2, p3, p4])])\n",
    "    polygons.append(Polygon([p1, p2, p3, p4]))\n",
    "    partition.append(P)\n",
    "    partition_vertices. append([p1, p2, p3, p4])\n",
    "\n",
    "# add markov partition to overall experiment dictionary\n",
    "baseline_partitions[\"vertical\"] = {}\n",
    "baseline_partitions[\"vertical\"][\"partition\"] = partition\n",
    "baseline_partitions[\"vertical\"][\"partition_vertices\"] = partition_vertices\n",
    "\n",
    "MultiPolygon(polygons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = 3\n",
    "n2 = 3\n",
    "x_offset = np.array([m_id / n1, 0])\n",
    "y_offset = np.array([0, m_id / n2])\n",
    "\n",
    "partition = []\n",
    "partition_vertices = []\n",
    "polygons = []\n",
    "\n",
    "for i in range(n1):\n",
    "    for j in range(n2):\n",
    "        p1 = u1 + i*x_offset + j*y_offset\n",
    "        p2 = p1 + x_offset\n",
    "        p3 = p2 + y_offset\n",
    "        p4 = p3 - x_offset\n",
    "\n",
    "        P = MultiPolygon([Polygon([p1, p2, p3, p4])])\n",
    "        polygons.append(Polygon([p1, p2, p3, p4]))\n",
    "        partition.append(P)\n",
    "        partition_vertices. append([p1, p2, p3, p4])\n",
    "\n",
    "# add markov partition to overall experiment dictionary\n",
    "baseline_partitions[\"equally_9\"] = {}\n",
    "baseline_partitions[\"equally_9\"][\"partition\"] = partition\n",
    "baseline_partitions[\"equally_9\"][\"partition_vertices\"] = partition_vertices\n",
    "\n",
    "MultiPolygon(polygons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = 4\n",
    "n2 = 3\n",
    "x_offset = np.array([m_id / n1, 0])\n",
    "y_offset = np.array([0, m_id / n2])\n",
    "\n",
    "partition = []\n",
    "partition_vertices = []\n",
    "polygons = []\n",
    "\n",
    "for i in range(n1):\n",
    "    for j in range(n2):\n",
    "        p1 = u1 + i*x_offset + j*y_offset\n",
    "        p2 = p1 + x_offset\n",
    "        p3 = p2 + y_offset\n",
    "        p4 = p3 - x_offset\n",
    "\n",
    "        P = MultiPolygon([Polygon([p1, p2, p3, p4])])\n",
    "        polygons.append(Polygon([p1, p2, p3, p4]))\n",
    "        partition.append(P)\n",
    "        partition_vertices. append([p1, p2, p3, p4])\n",
    "\n",
    "# add markov partition to overall experiment dictionary\n",
    "baseline_partitions[\"equally_12_1\"] = {}\n",
    "baseline_partitions[\"equally_12_1\"][\"partition\"] = partition\n",
    "baseline_partitions[\"equally_12_1\"][\"partition_vertices\"] = partition_vertices\n",
    "\n",
    "MultiPolygon(polygons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = 3\n",
    "n2 = 4\n",
    "x_offset = np.array([m_id / n1, 0])\n",
    "y_offset = np.array([0, m_id / n2])\n",
    "\n",
    "partition = []\n",
    "partition_vertices = []\n",
    "polygons = []\n",
    "\n",
    "for i in range(n1):\n",
    "    for j in range(n2):\n",
    "        p1 = u1 + i*x_offset + j*y_offset\n",
    "        p2 = p1 + x_offset\n",
    "        p3 = p2 + y_offset\n",
    "        p4 = p3 - x_offset\n",
    "\n",
    "        P = MultiPolygon([Polygon([p1, p2, p3, p4])])\n",
    "        polygons.append(Polygon([p1, p2, p3, p4]))\n",
    "        partition.append(P)\n",
    "        partition_vertices. append([p1, p2, p3, p4])\n",
    "\n",
    "# add markov partition to overall experiment dictionary\n",
    "baseline_partitions[\"equally_12_2\"] = {}\n",
    "baseline_partitions[\"equally_12_2\"][\"partition\"] = partition\n",
    "baseline_partitions[\"equally_12_2\"][\"partition_vertices\"] = partition_vertices\n",
    "\n",
    "MultiPolygon(polygons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"results/baseline_11_partitions.pkl\", \"wb\") as file:\n",
    "    pickle.dump(baseline_partitions, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with all 11 subset partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_candidates = np.linspace(start=0.05, stop=0.95, num=2)\n",
    "estimation_repetitions = 1\n",
    "experiment_results = {}\n",
    "\n",
    "for experiment_run in baseline_partitions.keys():\n",
    "    print(\"\\n---------------------------------------\\n\")\n",
    "    print(f\"Experiment run with {experiment_run} partition\")\n",
    "\n",
    "    partition = baseline_partitions[experiment_run][\"partition\"]\n",
    "    partition_vertices = baseline_partitions[experiment_run][\"partition_vertices\"]\n",
    "    markov_decision_process = MarkovDecisionProcess(dynamic_system, partition, partition_vertices=partition_vertices, target_state=target_state)\n",
    "\n",
    "    print(\"Estimate transition probabilities\")\n",
    "    n = len(partition)\n",
    "    total_time_estimation_results = np.zeros((estimation_repetitions, n))\n",
    "    num_iters_per_state_results = np.zeros((estimation_repetitions, n))\n",
    "\n",
    "    for i in range(estimation_repetitions): \n",
    "        start_time_estimation = time.time()\n",
    "        _, num_iters_per_state = markov_decision_process.estimate_probability_matrix_pi_method(c=c, tau=tau, max_sample_trials=1000)\n",
    "        total_time_estimation = time.time() - start_time_estimation\n",
    "        total_time_estimation_results[i, :] = total_time_estimation\n",
    "        num_iters_per_state_results[i, :] = num_iters_per_state\n",
    "\n",
    "    print(f\"Transition probability estimation took {round(np.mean(total_time_estimation_results), 2)}s on average with {round(np.var(total_time_estimation_results), 2)}s variance.\")\n",
    "    print(f\"Number of iterates per state {np.mean(num_iters_per_state_results, axis=0)} on average with variance {np.var(num_iters_per_state_results, axis=0)}\")\n",
    "\n",
    "    experiment_results[experiment_run] = {}\n",
    "    experiment_results[experiment_run][\"partition\"] = partition\n",
    "    experiment_results[experiment_run][\"partition_vertices\"] = partition_vertices\n",
    "    experiment_results[experiment_run][\"estimation_run_time\"] = total_time_estimation_results\n",
    "    experiment_results[experiment_run][\"num_iters_per_state\"] = num_iters_per_state_results\n",
    "    experiment_results[experiment_run][\"policy_evaluation\"] = {}\n",
    "\n",
    "    for gamma in gamma_candidates:\n",
    "        print(f\"\\nEvaluate policy with gamma={gamma}\")\n",
    "        experiment_results[experiment_run][\"policy_evaluation\"][gamma] = {}\n",
    "        markov_decision_process.gamma = gamma\n",
    "\n",
    "        epsilon = 10e-6 # 2 * np.finfo(float).eps\n",
    "        start_time_evaluation = time.time()\n",
    "        V, num_iters, convergence_info = markov_decision_process.policy_evaluation(markov_decision_process.g, epsilon=epsilon)\n",
    "        total_time_evaluation = time.time() - start_time_evaluation\n",
    "        print(f\"Policy evaluation took {total_time_evaluation}s\")\n",
    "        print(f\"Value function: {V}\")\n",
    "        print(f\"Number of iterations until convergence: {num_iters}\")\n",
    "\n",
    "        experiment_results[experiment_run][\"policy_evaluation\"][gamma][\"run_time\"] = total_time_evaluation\n",
    "        experiment_results[experiment_run][\"policy_evaluation\"][gamma][\"value_function\"] = V\n",
    "        experiment_results[experiment_run][\"policy_evaluation\"][gamma][\"num_iters\"] = num_iters\n",
    "        experiment_results[experiment_run][\"policy_evaluation\"][gamma][\"convergence_info\"] = convergence_info\n",
    "\n",
    "with open(f\"results/policy_evaluation_11_partition_results.pkl\", \"wb\") as file:\n",
    "    pickle.dump(experiment_results, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize experiment results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/policy_evaluation_11_partition_results.pkl\", \"rb\") as file:\n",
    "    experiment_results = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment_run in experiment_results.keys():\n",
    "    print(\"\\n---------------------------------------\\n\")\n",
    "    print(f\"Experiment run with {experiment_run} partition\")\n",
    "\n",
    "    for gamma in experiment_results[experiment_run][\"policy_evaluation\"]:\n",
    "        num_iters = experiment_results[experiment_run][\"policy_evaluation\"][gamma][\"num_iters\"]\n",
    "        convergence_info = experiment_results[experiment_run][\"policy_evaluation\"][gamma][\"convergence_info\"]\n",
    "        x_values = list(range(1, num_iters))\n",
    "\n",
    "        ax = sns.lineplot(x=x_values, y=convergence_info[\"max_dist\"][1:])\n",
    "        ax.set(xlabel=\"Iteration\", ylabel=\"MAX-DIST\")\n",
    "        plt.title(f\"Maximal convergence distance for gamma = {round(gamma, 2)}\")\n",
    "        plt.show()\n",
    "\n",
    "        ax = sns.lineplot(x=x_values, y=convergence_info[\"avg_dist\"][1:])\n",
    "        ax.set(xlabel=\"Iteration\", ylabel=\"L2-DIST\")\n",
    "        plt.title(f\"L2 convergence distance for gamma={round(gamma, 2)}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimation_convergence_results = []\n",
    "for experiment_run in experiment_results.keys():\n",
    "    estimation_run_time_avg = np.mean(experiment_results[experiment_run][\"estimation_run_time\"])\n",
    "    estimation_run_time_var = np.var(experiment_results[experiment_run][\"estimation_run_time\"])\n",
    "\n",
    "    sum_estimation_iters_avg = np.mean(np.sum(experiment_results[experiment_run][\"num_iters_per_state\"], axis=1))\n",
    "    sum_estimation_iters_var = np.var(np.sum(experiment_results[experiment_run][\"num_iters_per_state\"], axis=1))\n",
    "\n",
    "    avg_estimation_iters_avg = np.mean(np.mean(experiment_results[experiment_run][\"num_iters_per_state\"], axis=1))\n",
    "    avg_estimation_iters_var = np.var(np.mean(experiment_results[experiment_run][\"num_iters_per_state\"], axis=1))\n",
    "\n",
    "    max_estimation_iters_avg = np.mean(np.max(experiment_results[experiment_run][\"num_iters_per_state\"], axis=1))\n",
    "    max_estimation_iters_var = np.var(np.max(experiment_results[experiment_run][\"num_iters_per_state\"], axis=1))\n",
    "\n",
    "    estimation_convergence_result = {\n",
    "        \"partition_method\": experiment_run,\n",
    "        \"estimation_run_time_avg\": estimation_run_time_avg,\n",
    "        \"estimation_run_time_var\": estimation_run_time_var,\n",
    "        \"sum_estimation_iters_avg\": sum_estimation_iters_avg,\n",
    "        \"sum_estimation_iters_var\": sum_estimation_iters_var,\n",
    "        \"avg_estimation_iters_avg\": avg_estimation_iters_avg,\n",
    "        \"avg_estimation_iters_var\": avg_estimation_iters_var,\n",
    "        \"max_estimation_iters_avg\": max_estimation_iters_avg,\n",
    "        \"max_estimation_iters_var\": max_estimation_iters_var,\n",
    "    }\n",
    "    \n",
    "    estimation_convergence_results.append(estimation_convergence_result)\n",
    "\n",
    "estimation_convergence_results_df = pd.DataFrame(estimation_convergence_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimation_convergence_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_cols = [\"partition_method\", \"sum_estimation_iters_avg\", \"avg_estimation_iters_avg\", \"max_estimation_iters_avg\"] \n",
    "cols_naming = [\"Partition\", \"Sum of Iters\", \"Average Iters\", \"Max Iters\"]\n",
    "latex_table_str = estimation_convergence_results_df.to_latex(index=False, columns=relevant_cols, header=cols_naming, float_format=\"%.2f\", bold_rows=True)\n",
    "\n",
    "with open(\"results/latex_table_11_partitions_export.txt\", \"w\") as file:\n",
    "    file.write(latex_table_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_gamma = 0.95\n",
    "convergence_results = {}\n",
    "\n",
    "for experiment_run in experiment_results.keys():\n",
    "    num_iters = experiment_results[experiment_run][\"policy_evaluation\"][fixed_gamma][\"num_iters\"]\n",
    "    convergence_info = experiment_results[experiment_run][\"policy_evaluation\"][fixed_gamma][\"convergence_info\"]\n",
    "    x_values = list(range(1, 50))\n",
    "\n",
    "    plt.plot(x_values, convergence_info[\"max_dist\"][1:50], label=experiment_run)\n",
    "\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Max-DIST\")\n",
    "plt.title(f\"Maximal convergence distance for gamma = {fixed_gamma}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_gamma = 0.95\n",
    "convergence_results = {}\n",
    "\n",
    "for experiment_run in experiment_results.keys():\n",
    "    num_iters = experiment_results[experiment_run][\"policy_evaluation\"][fixed_gamma][\"num_iters\"]\n",
    "    convergence_info = experiment_results[experiment_run][\"policy_evaluation\"][fixed_gamma][\"convergence_info\"]\n",
    "    x_values = list(range(1, 50))\n",
    "\n",
    "    plt.plot(x_values, convergence_info[\"avg_dist\"][1:50], label=experiment_run)\n",
    "\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"L2-DIST\")\n",
    "plt.title(f\"L2 convergence distance for gamma = {fixed_gamma}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_gamma = 0.95\n",
    "num_iters_list = []\n",
    "\n",
    "for experiment_run in experiment_results.keys():\n",
    "    num_iters = experiment_results[experiment_run][\"policy_evaluation\"][fixed_gamma][\"num_iters\"]\n",
    "    num_iters_dict = {\"partition\": experiment_run, \"num_iters\": num_iters}\n",
    "    num_iters_list.append(num_iters_dict)\n",
    "\n",
    "num_iters_df = pd.DataFrame(num_iters_list)\n",
    "\n",
    "ax = sns.barplot(x=\"partition\", y=\"num_iters\", data=num_iters_df, hue=\"partition\", saturation=8, dodge=False)\n",
    "ax.set(xlabel=\"partition method\", ylabel=\"Number of Iterations\")\n",
    "plt.title(\"Policy Evaluation Convergence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iters_list = []\n",
    "\n",
    "for experiment_run in experiment_results.keys():\n",
    "    for gamma in experiment_results[experiment_run][\"policy_evaluation\"]:\n",
    "        num_iters = experiment_results[experiment_run][\"policy_evaluation\"][gamma][\"num_iters\"]\n",
    "        num_iters_dict = {\"partition\": experiment_run, \"gamma\": gamma, \"num_iters\": num_iters}\n",
    "        num_iters_list.append(num_iters_dict)\n",
    "\n",
    "num_iters_df = pd.DataFrame(num_iters_list)\n",
    "\n",
    "ax = sns.lineplot(data=num_iters_df, x=\"gamma\", y=\"num_iters\", hue=\"partition\")\n",
    "ax.set(xlabel=\"discount factor gamma\", ylabel=\"Number of Iterations\")\n",
    "plt.title(\"Policy Evaluation Convergence\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
